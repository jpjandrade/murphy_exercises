{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex 8.1 - Spam classification using Logistic Regression\n",
    "====================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as spio\n",
    "\n",
    "def load_data():\n",
    "    '''\n",
    "    Loads the data from the matlab file and returns\n",
    "    as numpy arrays with nice formatting\n",
    "    '''\n",
    "    A = spio.loadmat('../data/spam_data.mat')\n",
    "    X_train = A['Xtrain']\n",
    "    X_test = A['Xtest']\n",
    "    y_train = A['ytrain'].flatten().astype(int)\n",
    "    y_test = A['ytest'].flatten().astype(int)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def standarize_data(data):\n",
    "    '''\n",
    "    Returns data standarized so each column will have\n",
    "    mean 0 and unit variance\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    # Train\n",
    "    X_average = np.average(X_train, axis=0)\n",
    "    X_std = np.std(X_train, axis=0)\n",
    "    X_train = (X_train-X_average)/X_std\n",
    "    # test\n",
    "    X_average = np.average(X_test, axis=0)\n",
    "    X_std = np.std(X_test, axis=0)\n",
    "    X_test = (X_test-X_average)/X_std\n",
    "\n",
    "    data = X_train, X_test, y_train, y_test\n",
    "    return data\n",
    "\n",
    "\n",
    "def transform_log_scale(data):\n",
    "    '''\n",
    "    Transform the data to log, making sure it's not NaN!\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    X_train = np.log(X_train + 0.001)\n",
    "    X_test = np.log(X_test + 0.001)\n",
    "    data = X_train, X_test, y_train, y_test\n",
    "    return data\n",
    "\n",
    "\n",
    "def binarize_data(data):\n",
    "    '''\n",
    "    Returns binarized data so that each entry will be\n",
    "    just 0 or 1\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    X_train = (X_train > 0).astype(int)\n",
    "    X_test = (X_test > 0).astype(int)\n",
    "    data = X_train, X_test, y_train, y_test\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()\n",
    "data_orig = (X_train, X_test, y_train, y_test)\n",
    "data_standarized = standarize_data(data_orig)\n",
    "data_log_scale = transform_log_scale(data_orig)\n",
    "data_binarized = binarize_data(data_orig)\n",
    "\n",
    "all_data = [data_orig, data_standarized,\n",
    "            data_log_scale, data_binarized]\n",
    "data_names = ['original', 'standarized',\n",
    "              'log_scale', 'binarized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigm(x):\n",
    "    res = 1./(1 + np.exp(-x))\n",
    "    return res\n",
    "\n",
    "\n",
    "def jac(X, y, w, lamb):\n",
    "    mu = sigm(X.dot(w))\n",
    "    res = X.T.dot(mu - y) + 2*lamb*w\n",
    "    return res\n",
    "\n",
    "def gradient_descent(X, y, lamb):\n",
    "    steps = 20000\n",
    "    eta = 1./lamb\n",
    "    eta_final = 0.01/lamb\n",
    "    eta_step = (eta_final - eta)/steps\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.normal(size=n_features)\n",
    "    for t in range(steps):\n",
    "        grad = jac(X, y, w, lamb)\n",
    "        w = w - eta * grad\n",
    "        eta = eta + eta_step\n",
    "    return w\n",
    "    \n",
    "def logistic_regression_l2(data, lamb):\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    n_test = len(y_test)\n",
    "    X_train = np.hstack([np.ones((len(X_train),1)), X_train])\n",
    "    X_test = np.hstack([np.ones((len(X_test),1)), X_test])\n",
    "    w  = gradient_descent(X_train, y_train, lamb)\n",
    "    prob_predicted = sigm(X_test.dot(w))\n",
    "    y_predicted = ((2*prob_predicted - 1)>0).astype(int)\n",
    "    error_rate = np.sum(y_predicted != y_test)/n_test\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression error rate for original data: 0.59\n",
      "Logistic regression error rate for standarized data: 0.09\n",
      "Logistic regression error rate for log_scale data: 0.10\n",
      "Logistic regression error rate for binarized data: 0.09\n"
     ]
    }
   ],
   "source": [
    "lamb = 50.\n",
    "for data, data_name in zip(all_data, data_names):\n",
    "    error_rate = logistic_regression_l2(data, lamb)\n",
    "    print('Logistic regression error rate for %s data: %.2f' %\n",
    "          (data_name, error_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
